{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input'\n",
    "model_name = 'model-0.14135'\n",
    "raw_prediction_on_train_set = 'raw_pred_{}.pkl'.format(model_name)\n",
    "# this is saved in keras_train.py with \n",
    "# pickle.dump((ypred_train, ypred_valid, ytrain, yvalid), f)\n",
    "\n",
    "special_str = ''\n",
    "batch_method = 0\n",
    "prediction_filename = os.path.join(data_dir, '../output/keras_pred_{}{}_BM{}.csv'.format(model_name, special_str, str(batch_method)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "with open(prediction_filename.replace('.csv', '.pkl'), 'rb') as f:\n",
    "    test_filenames, ytest = pickle.load(f)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 17) (4000, 17) (16000, 17) (4000, 17)\n"
     ]
    }
   ],
   "source": [
    "with open(raw_prediction_on_train_set, 'rb') as f:\n",
    "    ypred_train, ypred_valid, ytrain, yvalid = pickle.load(f)\n",
    "\n",
    "print(ypred_train.shape, ypred_valid.shape, ytrain.shape, yvalid.shape)\n",
    "y_pred = np.concatenate((ypred_train, ypred_valid))\n",
    "y_true = np.concatenate((ytrain, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_i = (y_pred > .3).astype(int)\n",
    "#score = fbeta_score(y_true, y_pred_i, beta=2, average='samples')\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.random.rand(17)\n",
    "# thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proba_to_int(yproba, thresh):\n",
    "    y_pred_t = yproba.copy()\n",
    "    for i in range(y_pred_t.shape[1]):\n",
    "        y_pred_t[:,i] = yproba[:,i] > thresh[i]\n",
    "        #print(y_pred)\n",
    "    \n",
    "    y_pred_i = y_pred_t.astype(int)\n",
    "    return y_pred_i\n",
    "\n",
    "\n",
    "def fbeta_with_thresholds(thresh, y_true=None, y_pred=None):\n",
    "    if isinstance(thresh, float):\n",
    "        thresh = np.ones(17)*thresh\n",
    "    \n",
    "    y_pred_i = proba_to_int(y_pred, thresh)\n",
    "    \n",
    "    score = fbeta_score(y_true, y_pred_i, beta=2, average='samples')\n",
    "    #print(score)\n",
    "    return score*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbeta_with_thresholds(0.3, y_true, y_pred)\n",
    "# -0.88047779458491238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fbeta_with_thresholds(thresholds, y_true, y_pred)\n",
    "# #-0.7625872868255914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimize(fbeta_with_thresholds, x0=thresholds, args=(y_true, y_pred), tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████▎                                                     | 6/17 [00:11<00:21,  1.92s/it]c:\\users\\dingran\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:32<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "opt_thresh = list(np.ones(17)*.2)\n",
    "opt_score = fbeta_with_thresholds(opt_thresh, y_true, y_pred)\n",
    "for i in tqdm(range(len(opt_thresh))):\n",
    "    for t in np.arange(0.01, .9, .011):\n",
    "        tmp_thresh = opt_thresh.copy()\n",
    "        tmp_thresh[i]  = t\n",
    "        new_score = fbeta_with_thresholds(tmp_thresh, y_true, y_pred)\n",
    "        if new_score < opt_score :\n",
    "            opt_thresh[i] = t\n",
    "            opt_score = new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.887820732767\n",
      "[0.30699999999999994, 0.075999999999999984, 0.17499999999999996, 0.08699999999999998, 0.20000000000000001, 0.31799999999999995, 0.17499999999999996, 0.11999999999999997, 0.31799999999999995, 0.30699999999999994, 0.13099999999999998, 0.21899999999999997, 0.21899999999999997, 0.31799999999999995, 0.20799999999999996, 0.25199999999999995, 0.22999999999999995]\n"
     ]
    }
   ],
   "source": [
    "print(opt_score)\n",
    "print(opt_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▏                                 | 10/17 [00:18<00:13,  1.87s/it]c:\\users\\dingran\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:31<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# run again in reverse to check convergence\n",
    "ilist = list(range(len(opt_thresh)))\n",
    "for i in tqdm(ilist[::-1]):\n",
    "    # print(i)\n",
    "    for t in np.arange(0.01, .9, .011):\n",
    "        tmp_thresh = opt_thresh.copy()\n",
    "        tmp_thresh[i]  = t\n",
    "        new_score = fbeta_with_thresholds(tmp_thresh, y_true, y_pred)\n",
    "        if new_score < opt_score :\n",
    "            opt_thresh[i] = t\n",
    "            opt_score = new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.887825809243\n",
      "[0.30699999999999994, 0.075999999999999984, 0.17499999999999996, 0.08699999999999998, 0.20000000000000001, 0.39499999999999991, 0.17499999999999996, 0.11999999999999997, 0.31799999999999995, 0.30699999999999994, 0.13099999999999998, 0.21899999999999997, 0.21899999999999997, 0.31799999999999995, 0.20799999999999996, 0.25199999999999995, 0.22999999999999995]\n"
     ]
    }
   ],
   "source": [
    "print(opt_score)\n",
    "print(opt_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_i = proba_to_int(ytest, opt_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tag_translation import train_label, tags_to_vec, map_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = map_predictions(ytest_i)\n",
    "predicted_labels_str = [' '.join(x) for x in predicted_labels]\n",
    "df = pd.DataFrame({'image_name': test_filenames, 'tags': predicted_labels_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_filename = os.path.join(data_dir, '../output/keras_pred_{}_opt.csv'.format(model_name))\n",
    "df.to_csv(prediction_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}